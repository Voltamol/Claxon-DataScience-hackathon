


import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import os
import warnings
import math
import plotly.graph_objects as go
import plotly.express as px
sns.set_style(style="darkgrid")
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import BernoulliNB
#from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report





warnings.filterwarnings("ignore")





current=os.getcwd()
root=os.path.dirname(current)


print(os.listdir(root))





path=os.path.join(root,'data','data_science_competition_2024.csv')





df=pd.read_csv(path)





df.head()











df=pd.read_csv(path,index_col=0)


df.head()





rows,columns=df.shape


print("rows:{}\ncolumns:{}".format(rows,columns))





df.isnull().sum()





nulls=df.isnull().sum()


nulls=nulls[nulls>0]


nulls





null_columns=nulls.index.tolist()
null_columns


subset=df[null_columns]
subset.head()





subset.country.value_counts()














subset['country']=subset.country.str.title()





subset.country.value_counts()





subset['country']=subset.country.apply(lambda x:'Zimbabwe' if x=='Zim' else x)





subset.country.value_counts()


rows





subset['country']=subset.country.fillna('Zimbabwe')


df['country']=subset.country





subset=subset.drop('country',axis=1)


subset.head()





subset.describe().transpose()


subset.job.value_counts()





df['job'].replace('Data Scintist','Data Scientist',inplace=True)
df['job'].replace('SoftwareDeveloper','Software Developer',inplace=True)





set(subset.location.tolist())





subset['location']=subset.location.str.strip()


set(subset.location.tolist())





subset.location.value_counts()














subset.location.nunique()


subset.job.nunique()








df.drop(['age.1','sex','number_of_defaults.1'],axis=1,inplace=True)





missing = subset.columns.tolist()














# creatin encoders
enc1=LabelEncoder()
enc2=LabelEncoder()
# fitting encoders
enc1.fit(df[missing[0]].dropna())# adding dropna at the end to ensure that NaN is not encoded together with valid data
enc2.fit(df[missing[1]].dropna())






labels1=enc1.classes_
labels2=enc2.classes_


labels1


labels2





missing





encodings1=enc1.transform(labels1)
encodings2=enc2.transform(labels2)





job_dict=dict(zip(labels1,encodings1))
location_dict=dict(zip(labels2,encodings2))


job_dict


encode_jobs=lambda job:job_dict.get(job)
encode_locations=lambda location:location_dict.get(location)


# transforming features to numeric
subset[missing[0]]=subset[missing[0]].apply(encode_jobs)
subset[missing[1]]=subset[missing[1]].apply(encode_locations)








numeric=df.describe().columns.tolist()





numeric=df[numeric]


numeric=numeric.join(subset)


numeric.head()


numeric.isnull().sum()





imputer = KNNImputer()
transformed = imputer.fit_transform(numeric)


#placeholder


#converting transformed to a dataframe to access the imputed columns easily
transformed=pd.DataFrame(transformed,columns=numeric.columns)


# replacing features with missing values by ones without any missing values
for feature in missing:
    df[feature] = transformed[feature]





df[missing].head()





df.isnull().sum()








for feature in missing:
    df[feature]=df[feature].apply(math.floor)





df[missing].dtypes





df[missing[0]]=enc1.inverse_transform(df[missing[0]])
df[missing[1]]=enc2.inverse_transform(df[missing[1]])





df.isnull().sum()





df.head()











df.head()








df.describe()











numeric=df.describe().columns.tolist()


sns.heatmap(df[numeric].corr(),cmap='magma',annot=True)











df[numeric].plot.box()
plt.xticks(rotation=45);# semicolon prevents a bunch of unwated text from appearing above graph


sns.pairplot(df[numeric])














numeric





x,y,z=df['loan_amount'],df['interest_rate'],df['number_of_defaults']

fig = go.Figure(data=[go.Scatter3d(
    x=x,
    y=y,
    z=z,
    mode='markers',
    marker=dict(
        size=5,
        color=z,
        colorscale='Viridis',
        opacity=0.8
    )
)])


fig.update_layout(
    scene={
        'xaxis_title': 'loan_amount',
        'yaxis_title': 'interest_rate',
        'zaxis_title': 'number_of_defaults'
    },
    margin={'l': 0, 'r': 0, 'b': 0, 't': 0},
    title='Scatter Plot of loan_amount,interest_rate and number of defaults'
)

fig.show()


x,y,z=df['loan_amount'],df['interest_rate'],df['salary']

fig = go.Figure(data=[go.Scatter3d(
    x=x,
    y=y,
    z=z,
    mode='markers',
    marker=dict(
        size=5,
        color=z,
        colorscale='Viridis',
        opacity=0.8
    )
)])


fig.update_layout(
    scene={
        'xaxis_title': 'loan_amount',
        'yaxis_title': 'interest_rate',
        'zaxis_title': 'salary'
    },
    margin={'l': 0, 'r': 0, 'b': 0, 't': 0},
    title='Scatter Plot of loan_amount,interest_rate and salary'
)

fig.show()









sns.countplot(data=df,x='job')
plt.xticks(rotation=45);


sns.countplot(data=df,x='location')
plt.xticks(rotation=90);


df.columns


px.scatter(
    df,
    x='salary',
    y='outstanding_balance',
    size='age',
    animation_frame='disbursemet_date',
    hover_data='job',
    facet_col='Loan Status',
    color='location'
)


#df.to_csv("final_cleaned.csv",index=False)





encoder=LabelEncoder()


for feature in df.select_dtypes("object"):
    df[feature]=encoder.fit_transform(df[feature])


target='Loan Status'
features=df.columns.tolist()
x=df.drop(target,axis=1)
y=df[target]


mutual_info = mutual_info_classif(x, y)

sorted_indices = np.argsort(mutual_info)[::-1]

for i in sorted_indices:
    print(f"Feature: {features[i]}, Mutual Information: {mutual_info[i]:.3f}")





x.drop(['loan_id','country','disbursemet_date'],axis=1,inplace=True)


x.head()


job_idx=3
location_idx=4


enc1.classes_


encoders={
    job_idx:enc1,
    location_idx:enc2
}


scaler=MinMaxScaler()
scaler.fit(x)


scaled=scaler.transform(x)


x_train,x_test,y_train,y_test=train_test_split(scaled,y,test_size=0.2,random_state=42)








param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

lr = LogisticRegression()
grid_search = GridSearchCV(lr, param_grid, cv=5)
grid_search.fit(x_train, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_


best_params


lr=best_model





param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

rfc = RandomForestClassifier()
grid_search = GridSearchCV(rfc, param_grid, cv=5)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_


best_params


rfc=best_model


param_grid = {
    'alpha': [0.1, 1, 10],
    'binarize': [0.0, 0.5, 1.0]
}
bnb = BernoulliNB()
grid_search = GridSearchCV(bnb, param_grid, cv=5)
grid_search.fit(x_train, y_train)
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_


best_params


bnb=best_model


param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

svc = SVC()
grid_search = GridSearchCV(svc, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_


best_params


svc=best_model



